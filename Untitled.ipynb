{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T13:24:38.230717Z",
     "start_time": "2018-06-18T13:24:37.088986Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T13:27:18.501582Z",
     "start_time": "2018-06-18T13:27:18.457707Z"
    }
   },
   "outputs": [],
   "source": [
    "# rg\n",
    "\n",
    "def radius_of_gyration(positions):\n",
    "    \"\"\"\n",
    "    position : tuple\n",
    "        A tuple (lat, lon) with the latitude and longitude of the antenna,\n",
    "        encoded as floating point numbers.\n",
    "\n",
    "    Returns the radius of gyration, the *equivalent distance* of the mass from\n",
    "    the center of gravity, for all visited places. [GON2008]_\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [GON2008] Gonzalez, M. C., Hidalgo, C. A., & Barabasi, A. L. (2008).\n",
    "        Understanding individual human mobility patterns. Nature, 453(7196),\n",
    "        779-782.\n",
    "    \"\"\"\n",
    "    d = Counter(positions)\n",
    "    sum_weights = sum(d.values())\n",
    "    positions = list(d.keys())  # Unique positions\n",
    "\n",
    "    if len(positions) == 0:\n",
    "        return None\n",
    "\n",
    "    barycenter = [0, 0]\n",
    "    for pos, t in d.items():\n",
    "        barycenter[0] += pos[0] * t\n",
    "        barycenter[1] += pos[1] * t\n",
    "\n",
    "    barycenter[0] /= sum_weights\n",
    "    barycenter[1] /= sum_weights\n",
    "\n",
    "    r = 0.\n",
    "    for pos, t in d.items():\n",
    "        r += float(t) / sum_weights * \\\n",
    "            great_circle_distance(barycenter, pos) ** 2\n",
    "    return math.sqrt(r)\n",
    "\n",
    "def great_circle_distance(pt1, pt2):\n",
    "    \"\"\"\n",
    "    Return the great-circle distance in kilometers between two points,\n",
    "    defined by a tuple (lat, lon).\n",
    "    Examples\n",
    "    --------\n",
    "    >>> brussels = (50.8503, 4.3517)\n",
    "    >>> paris = (48.8566, 2.3522)\n",
    "    >>> great_circle_distance(brussels, paris)\n",
    "    263.9754164080347\n",
    "    \"\"\"\n",
    "    r = 6371.\n",
    "\n",
    "    delta_latitude = math.radians(pt1[0] - pt2[0])\n",
    "    delta_longitude = math.radians(pt1[1] - pt2[1])\n",
    "    latitude1 = math.radians(pt1[0])\n",
    "    latitude2 = math.radians(pt2[0])\n",
    "\n",
    "    a = math.sin(delta_latitude / 2) ** 2 + math.cos(latitude1) * math.cos(latitude2) * math.sin(delta_longitude / 2) ** 2\n",
    "    return r * 2. * math.asin(math.sqrt(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T13:42:43.029262Z",
     "start_time": "2018-06-18T13:42:42.275515Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuhuimin/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2698: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "corpus=pd.read_csv('./category/node.csv',index_col=0,names=['user_id','app'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T13:43:18.235868Z",
     "start_time": "2018-06-18T13:43:18.227224Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus=corpus['app'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T13:44:46.835286Z",
     "start_time": "2018-06-18T13:44:46.830938Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['微信', '手机腾讯网', 'QQ', ..., '搜搜地图', 'QQ欢乐斗地主', 'QQ音乐'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T13:44:17.333444Z",
     "start_time": "2018-06-18T13:44:17.314348Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-dcf11696e4c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 300 dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#                 workers=multiprocessing.cpu_count(), hs=1, negative=0) # negative sampling, hs: hierarchical softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mword_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mword_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./book_vec_300_dims.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_train_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You can't pass a generator as the sentences argument. Try an iterator.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m             self.train(\n\u001b[1;32m    337\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m         \"\"\"\n\u001b[1;32m    479\u001b[0m         total_words, corpus_count = self.vocabulary.scan_vocab(\n\u001b[0;32m--> 480\u001b[0;31m             sentences, progress_per=progress_per, trim_rule=trim_rule)\n\u001b[0m\u001b[1;32m    481\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         report_values = self.vocabulary.prepare_vocab(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[0;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m   1163\u001b[0m                     \u001b[0msentence_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m                 )\n\u001b[0;32m-> 1165\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1166\u001b[0m                 \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m             \u001b[0mtotal_words\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "# 300 dimensions\n",
    "model = Word2Vec(corpus, size=300, window=10, min_count=5,\n",
    "                workers=multiprocessing.cpu_count(), hs=1, negative=0) # negative sampling, hs: hierarchical softmax\n",
    "word_vectors = model.wv\n",
    "word_vectors.save_word2vec_format('./book_vec_300_dims.txt')\n",
    "word_vectors = KeyedVectors.load_word2vec_format('./book_vec_200_dims.txt')\n",
    "\n",
    "vector = model.wv['computer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rg\n",
    "\n",
    "for vector in word_vectors.vocab.values():\n",
    "    # reduce the dimensions from 300 to 2\n",
    "    # or use 2-dimensions when training\n",
    "    radius_of_gyration(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T13:47:47.272505Z",
     "start_time": "2018-06-18T13:47:43.516977Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuhuimin/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'./localdata/all_attenion_clean.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-127e01807f5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# In[ ]:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./localdata/all_attenion_clean.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'./localdata/all_attenion_clean.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from collections import Counter\n",
    "import scipy\n",
    "import time\n",
    "import powerlaw\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.font_manager as font_manager\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "get_ipython().magic('matplotlib inline')\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import linear_model\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "path = '/System/Library/Fonts/Hiragino Sans GB W3.ttc'\n",
    "fontprop = font_manager.FontProperties(fname=path)\n",
    "\n",
    "matplotlib.style.use('_classic_test')\n",
    "matplotlib.rc(\"figure\", facecolor=\"white\")\n",
    "# matplotlib.style.use('seaborn')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "df = pd.read_csv('./localdata/all_attenion_clean.csv')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "pd.Series(df.groupby('uid')['book_id'])[:10]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "corpus=[]\n",
    "groupbySeries = pd.Series(df.groupby('uid')['book_id'])\n",
    "for i in range(len(groupbySeries)):\n",
    "    corpus.append(list(groupbySeries[i][1].values))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "corpus[0][:10]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "model = Word2Vec(corpus, size=100, window=5, min_count=1,\n",
    "                workers=multiprocessing.cpu_count(), hs=1, negative=0) # negative sampling, hs: hierarchical softmax\n",
    "word_vectors = model.wv\n",
    "word_vectors.save_word2vec_format('./book_vec.txt')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format('./book_vec.txt')\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "vectors = [word_vectors.wv[book] for book in word_vectors.vocab]\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "len(vectors)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "vectors = [tuple(i) for i in vectors]\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "vectors[:5]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "dimensions = 300\n",
    "def distance(a, b):\n",
    "    return math.sqrt(a-b)\n",
    "\n",
    "def radius_of_gyration(positions):\n",
    "    d = Counter(positions)\n",
    "    sum_weights = sum(d.values())\n",
    "    positions = list(d.keys())  # Unique positions\n",
    "\n",
    "    if len(positions) == 0:\n",
    "        return None\n",
    "\n",
    "    barycenter = [0] * dimensions\n",
    "    for pos, t in d.items():\n",
    "        for i in dimensions:\n",
    "            barycenter[i] += pos[i] * t\n",
    "\n",
    "    for i in dimensions:\n",
    "        barycenter[i] /= sum_weights\n",
    "\n",
    "    r = 0.\n",
    "    for pos, t in d.items():\n",
    "        r += float(t) / sum_weights *                 np.linalg.norm(np.array(barycenter)-np.array(pos)) ** 2\n",
    "    return math.sqrt(r)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[191]:\n",
    "\n",
    "\n",
    "\n",
    "# In[192]:\n",
    "\n",
    "rg = []\n",
    "for name, group in df.groupby('uid')['book_id']:\n",
    "    userBookLocations=[]\n",
    "#     print(name, list(group.values))\n",
    "#     print(groupbySeries[i][0], list(groupbySeries[i][1].values))\n",
    "    for book in list(group.values):\n",
    "        userBookLocations.append(tuple(word_vectors.wv[book]))\n",
    "    rg.append(radius_of_gyration(userBookLocations))\n",
    "\n",
    "\n",
    "# In[193]:\n",
    "\n",
    "plt.hist(rg)\n",
    "\n",
    "\n",
    "# In[194]:\n",
    "\n",
    "len(rg)\n",
    "\n",
    "\n",
    "# In[195]:\n",
    "\n",
    "\n",
    "\n",
    "# In[196]:\n",
    "\n",
    "mu = np.mean(rg)\n",
    "sigma = np.std(rg)\n",
    "count, bins, ignored = plt.hist(rg, 40, normed=True)\n",
    "plt.xlabel('$Radius\\ of\\ Gyration(log)$', size=20)\n",
    "plt.ylabel('$P(R_g)$', size=20)\n",
    "plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) * \n",
    "         np.exp( - (bins - mu)**2 / (2 * sigma**2) ),\n",
    "         linewidth=2, color='r')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[132]:\n",
    "\n",
    "rg_log = []\n",
    "for i in rg:\n",
    "    if i >= 1:\n",
    "        rg_log.append(np.log(i))\n",
    "\n",
    "\n",
    "# In[133]:\n",
    "\n",
    "mu = np.mean(rg_log)\n",
    "sigma = np.std(rg_log)\n",
    "count, bins, ignored = plt.hist(rg_log, 40, normed=True)\n",
    "plt.xlabel('$Radius\\ of\\ Gyration(log)$', size=20)\n",
    "plt.ylabel('$P(R_g)$', size=20)\n",
    "plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) * \n",
    "         np.exp( - (bins - mu)**2 / (2 * sigma**2) ),\n",
    "         linewidth=2, color='r')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[104]:\n",
    "\n",
    "a = pd.read_csv('./users1000.csv', index_col=0)\n",
    "a['RadiusofGyration'] = rg\n",
    "a.to_csv('./users1000.csv', index=True)\n",
    "\n",
    "\n",
    "# In[103]:\n",
    "\n",
    "a\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "def train_word2vec(corpus, dimensions=200, filename=None):\n",
    "    model = Word2Vec(corpus, size=200, window=10, min_count=5,\n",
    "                     workers=multiprocessing.cpu_count(), hs=1,\n",
    "                     negative=0)  # negative sampling, hs: hierarchical softmax\n",
    "    word_vectors = model.wv\n",
    "\n",
    "    if filename != None:\n",
    "        # Save the model\n",
    "        word_vectors.save_word2vec_format(filename)\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.font_manager as font_manager\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = '/System/Library/Fonts/Hiragino Sans GB W3.ttc'\n",
    "fontprop = font_manager.FontProperties(fname=path)\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "def visualize_word_vectors(word_vectors):\n",
    "    vec_dict = dict()\n",
    "    for book_id in vocab_list[:60]:\n",
    "        vec = word_vectors.word_vec(book_id)\n",
    "        vec_dict[book_id] = vec\n",
    "\n",
    "    color_dict = dict(matplotlib.colors.BASE_COLORS, **matplotlib.colors.CSS4_COLORS)\n",
    "    color_list = list(color_dict.keys())[15:]\n",
    "    print(color_list[:20])\n",
    "\n",
    "    X_tsne = TSNE(learning_rate=10).fit_transform(list(vec_dict.values()))\n",
    "\n",
    "    X_tsne_dict = dict()\n",
    "    for book in vec_dict.keys():\n",
    "        X_tsne_dict[book] = X_tsne[list(vec_dict.keys()).index(book)]\n",
    "\n",
    "    # 可以按照类别标出颜色\n",
    "\n",
    "    plt.figure(figsize=(30, 20))\n",
    "\n",
    "    for category in CateId2Cate.keys():\n",
    "        for book_id in X_tsne_dict.keys():\n",
    "            book_category = bookId2Cate[book_id]\n",
    "            if book_category == category:\n",
    "                color = color_list[CateId2Cate[category]]\n",
    "                plt.scatter(X_tsne_dict[book_id][0], X_tsne_dict[book_id][1], c=color, label=category, s=250, alpha=0.4)\n",
    "                plt.text(X_tsne_dict[book_id][0], X_tsne_dict[book_id][1], bookId2Name[book_id],\n",
    "                         fontproperties=fontprop, fontsize=20)\n",
    "\n",
    "    plt.legend(loc=3, prop=fontprop)\n",
    "    # plt.savefig('./200-dim-2.png', dpi=200)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.font_manager as font_manager\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "import datetime\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "path = '/System/Library/Fonts/Hiragino Sans GB W3.ttc'\n",
    "fontprop = font_manager.FontProperties(fname=path)\n",
    "\n",
    "# data_dir = '/Users/zhicongchen/Desktop/datasets/baidureading/'\n",
    "\n",
    "data_dir = './baidureading/'\n",
    "\n",
    "book_df = pd.read_csv(data_dir + 'book_info.csv')\n",
    "\n",
    "bookId = book_df['book_id'].values\n",
    "\n",
    "# bookName = book_df['book_name'].values\n",
    "# bookCate = book_df['category'].values\n",
    "# bookPrice = book_df['price'].values\n",
    "# bookCateId = book_df['categoryid'].values\n",
    "# CateId2Cate = dict(book_df[['category', 'categoryid']].drop_duplicates().values)\n",
    "# bookId2Name = dict(book_df[['book_id', 'book_name']].drop_duplicates().values)\n",
    "# bookId2Price = dict(book_df[['book_id', 'price']].drop_duplicates().values)\n",
    "# bookId2Cate = dict(book_df[['book_id', 'category']].drop_duplicates().values)\n",
    "\n",
    "bookId2Sale = dict(book_df[['book_id', 'salecount']].drop_duplicates().values)\n",
    "\n",
    "\n",
    "# all_attention = pd.read_csv(data_dir + 'attentionflow/all_attenion.csv')\n",
    "\n",
    "def attentionflow2corpus(all_attention):\n",
    "    reading_flow = all_attention.groupby('uid').groups\n",
    "    print(\"Num of users: \", len(reading_flow))\n",
    "    corpus = dict()\n",
    "    for user in reading_flow.keys():\n",
    "        corpus[user] = list(all_attention.iloc[reading_flow[user]]['book_id'].drop_duplicates().values)\n",
    "\n",
    "    # Dump corpus into file\n",
    "    f = open('corpus_dict.txt', 'wb')\n",
    "    pickle.dump(corpus, f)\n",
    "    f.close()\n",
    "    return corpus\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Read corpus from file\n",
    "    f = open('corpus_list.txt', 'rb')\n",
    "    corpus = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    # Load the model\n",
    "    # word_vectors = KeyedVectors.load_word2vec_format('./book_vec_200_dims.txt')\n",
    "\n",
    "    scores = []\n",
    "    dimensions = np.arange(1600, 5000, 50)\n",
    "    for s in dimensions:\n",
    "        print(\"Dimensions: %d\" % s)\n",
    "        start_time = datetime.datetime.now()\n",
    "        model = Word2Vec(corpus, size=s, window=10, min_count=5,\n",
    "                         workers=multiprocessing.cpu_count(), hs=1,\n",
    "                         negative=0)  # negative sampling, hs: hierarchical softmax\n",
    "        word_vectors = model.wv\n",
    "        vocab_list = list((set(bookId) & set(word_vectors.vocab.keys())))\n",
    "\n",
    "        # Load data\n",
    "        y = np.array([bookId2Sale[book_id] for book_id in vocab_list])\n",
    "        x = np.array([word_vectors.word_vec(word) for word in vocab_list])\n",
    "\n",
    "        r2 = sm.OLS(y, x).fit().rsquared\n",
    "        print(\"R2: \", r2)\n",
    "        scores.append(r2)\n",
    "\n",
    "        end_time = datetime.datetime.now()\n",
    "        print(\"Time Consumed: \", end_time - start_time)\n",
    "\n",
    "    print(scores)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(scores, 'o-')\n",
    "    plt.xticks(range(len(dimensions)), dimensions)\n",
    "    plt.xlabel('$Dimensions$', fontsize=15)\n",
    "    plt.ylabel('$R^2$', fontsize=15)\n",
    "    plt.title('OLS Regression', fontsize=18)\n",
    "    plt.savefig('./OLS_regression.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T14:02:15.223616Z",
     "start_time": "2018-09-28T14:02:14.553144Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuhuimin/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/xuhuimin/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T09:13:57.478474Z",
     "start_time": "2018-09-28T07:41:08.003773Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20131201\n",
      "20131202\n",
      "20131203\n",
      "20131204\n",
      "20131205\n",
      "20131206\n",
      "20131207\n",
      "20131208\n",
      "20131209\n",
      "20131210\n",
      "20131211\n",
      "20131212\n",
      "20131213\n",
      "20131214\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-a55f4675da99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.del.tar.gz'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/xuhuimin/Documents/lab/labdata/beijingmobile/node.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'a+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   1401\u001b[0m                                      \u001b[0mdoublequote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m                                      escapechar=escapechar, decimal=decimal)\n\u001b[0;32m-> 1403\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1590\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mwriter_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1592\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1691\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1693\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1695\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m   1705\u001b[0m                                   \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m                                   \u001b[0mdate_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1707\u001b[0;31m                                   quoting=self.quoting)\n\u001b[0m\u001b[1;32m   1708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1709\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcol_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mto_native_types\u001b[0;34m(self, slicer, na_rep, float_format, decimal, quoting, **kwargs)\u001b[0m\n\u001b[1;32m   1608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1609\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1610\u001b[0;31m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1611\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path = '/Volumes/My Book/data/phone/流量数据/gprs_bh_'\n",
    "for i in range(20131201, 20131232):\n",
    "    print(i)\n",
    "    filename=str(i)\n",
    "    data=pd.read_csv(path+filename+'.del.tar.gz',usecols=[0,5,6,9,10,11])\n",
    "    with open('/Users/xuhuimin/Documents/lab/labdata/beijingmobile/node.csv','a+') as f:\n",
    "        data.to_csv(f,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-28T16:00:52.346281Z",
     "start_time": "2018-09-28T14:07:02.299674Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20131214\n",
      "20131215\n",
      "20131216\n",
      "20131217\n",
      "20131218\n",
      "20131219\n",
      "20131220\n",
      "20131221\n",
      "20131222\n",
      "20131223\n",
      "20131224\n",
      "20131225\n",
      "20131226\n",
      "20131227\n",
      "20131228\n",
      "20131229\n",
      "20131230\n",
      "20131231\n"
     ]
    }
   ],
   "source": [
    "path = '/Volumes/My Book/data/phone/流量数据/gprs_bh_'\n",
    "for i in range(20131214, 20131232):\n",
    "    print(i)\n",
    "    filename=str(i)\n",
    "    data=pd.read_csv(path+filename+'.del.tar.gz',usecols=[0,5,6,9,10,11])\n",
    "    with open('/Users/xuhuimin/Documents/lab/labdata/beijingmobile/node.csv','a+') as f:\n",
    "        data.to_csv(f,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
